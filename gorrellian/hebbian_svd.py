"""
Incremental PCA, SVD using Hebbian updates (from data only).

  PCA problem:
  Given many data vectors x, find the eigenvectors of the correlation matrix R,
  where E(xx') = R.

  SVD problem:
  Given many data vector pairs (a,b), where a=Mb, find the singular vectors of M.

  References:

  Zhang, “Complex-Valued Neural Networks“, 2003.
    https://www.mbari.org/wp-content/uploads/2016/01/Zhang_bookchapter_2003.pdf

  G. Gorrell, “Generalized Hebbian algorithm for incremental singular value
  decomposition in natural language processing,” EACL 2006, 11st Conference of
  the European Chapter of the Association for Computational Linguistics,
  Proceedings of the Conference, April 3-7, 2006, Trento, Italy (D. McCarthy
  and S. Wintner, eds.), The Association for Computer Linguistics, 2006.

"""

import os
import jax

from jax import jit
from jax.numpy import array, outer, eye, zeros, tril, triu, sqrt, isnan
from jax.numpy.linalg import svd, eig, inv, norm

from time import time, strftime, gmtime


@jit
def couter(a, b):
    """
    Complex outer product of a and b: a x b^T.
    """
    return outer(a, b.conjugate())


# TODO: rewrite and test PCA functions
def pca_update(W, x, eta=1e-3):
    """
    Incremental update to approximate eigenvector matrix W of correlation matrix R
      with one data vector x. Returns updated W.
    See Zhang's book and the Wikipedia page on GHA.
    """
    y = conj(W.T) @ x
    return W + eta * ((couter(x, x) @ W) - (W @ triu(couter(y, y))))


def pca_solve(gen_vector, iterations=100000, verbose=False):
    """
    Solve iteratively for the eigenvectors of the correlation matrix.
    Exponential decay d on learning rate eta.
    """
    v = gen_vector()
    W = eye(len(v))
    for n in range(iterations):
        v = gen_vector()
        if verbose and n % 1000 == 0:
            print(f"{n:07d}, {W[..., 1]}    ", end='\r')
        W = pca_update(W, v, eta=eta_decayed)
    return W


def pca_test(A, **kwargs):
    R = A @ conj(A.T)
    L_ref, W_ref = eig(R)
    W0 = eye(R.shape[0])
    W = pca_solve(rng.standard_normal([N, N]), **kwargs)
    L = (inv(W) @ R @ W).diagonal()
    print("A")
    print(A)
    print()
    print("R")
    print(R)
    print()
    print("W (ref)")
    print(W_ref)
    print()
    print("W (GHA)")
    print(W)
    print()
    print("R W (ref)")
    print(R @ W_ref)
    print()
    print("R W (GHA)")
    print(R @ W)
    print()
    print("L (ref)")
    print(L_ref)
    print()
    print("L (GHA)")
    print(L)
    print()


@jit
def svd_update(U, V, yasq, ybsq, a, b, eta=1e-3, iterations=100):
    """
    Incremental update to approximate SVD matrices U, V of operator A
      with one data vector pair a, b. Returns updated U, V.
    See Gorrell's paper.
    """
    ya = U.T.conjugate() @ a
    yb = V.T.conjugate() @ b
    dU = couter(a, yb) - (U @ triu(couter(ya, yb)))
    dV = couter(b, ya) - (V @ triu(couter(yb, ya)))
    h = jax.numpy.min(array([eta, eta/norm(dU), eta/norm(dV)]))
    U += h * dU
    V += h * dV
    yasq += h * (ya.real * ya.real - yasq)
    ybsq += h * (yb.real * yb.real - ybsq)
    return U, V, yasq, ybsq


def svd_solve(gen_pair, L=5, eta=1e-3, C_barrier=0.1, max_iter=1e7, C_iter=100):
    """
    Solve iteratively for the first L singular vectors of A,
    where gen_pair is a function that returns data pairs generated by A:
        a, b, key = gen_pair(key)

    Keep going until basis seems to have converged.
    """
    key = jax.random.PRNGKey(10)
    a, b, key = gen_pair(key)
    N = len(a)
    M = len(b)
    # initial guess for basis U, V, svs
    U = eye(N, L) * eta
    V = eye(M, L) * eta
    yasq = zeros(L)
    ybsq = zeros(L)
    if verbose:
        print(f"eta_max: {eta}, N: {N}, M: {M}, L: {L}")
        t0 = time()
    C_last = 100 * max(N, M)
    C = 10 * max(N, M)
    iter = 0
    # terminate on orthogonality convergence or failing convergence
    while (C_last > C or C < C_barrier) and iter < max_iter:
        a, b, key = gen_pair(key)
        h = eta
        U, V, yasq, ybsq = svd_update(U, V, yasq, ybsq, a, b, eta=h, iterations=C_iter)
        iter += 1
        if iter % C_iter == 0:
            # C, rough convergence criterion based on approaching unitary U, V
            C_last = C
            C = norm(U.conjugate().T @ U - eye(L)) + norm(V.conjugate().T @ V - eye(L))
            if verbose:
                # U'U and V'V should approach I with convergence; print the diagonals.
                print(f"  #{iter:07d}   ", end='')
                print(f"C: {C:05.4f}   ", end='')
                print(f"converged σs: {L-C:03.2f}   ", end='')
                print(f"σs: {sqrt(yasq / ybsq)[:L]}   ", end='')
                print(f"wall time: {strftime('%H:%M:%S', gmtime(time()-t0))}      ", end='\r')
    return {"U": U, "V": V, "S": sqrt(yasq / ybsq)}


def svd_test(gen_pair, **kwargs):
    with jax.numpy.printoptions(precision=2):
        UVS = svd_solve(gen_pair, **kwargs)
        U = UVS["U"]
        V = UVS["V"]
        S = UVS["S"]
        with open('../data/U.npy', 'wb') as f:
            jax.numpy.save(f, U)
        with open('../data/V.npy', 'wb') as f:
            jax.numpy.save(f, V)
        with open('../data/S.npy', 'wb') as f:
            jax.numpy.save(f, S)
        print()
        print("saved result.")
        print()
        return UVS
